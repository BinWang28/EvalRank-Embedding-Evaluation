

| Word Embedding (cos) | EvalRank (MRR) | Hits1 | Hits3 |
| :--- | :---: | :---: | :---: |
| toy_emb.txt | 3.18 | 1.18 | 3.54 |
| [glove.840B.300d.txt](https://nlp.stanford.edu/projects/glove/) | 13.15 | 4.66 | 15.72 |
| [GoogleNews-vectors-negative300.txt](https://code.google.com/archive/p/word2vec/) | 12.88 | 4.57 | 14.35 |
| [crawl-300d-2M.vec](https://fasttext.cc/docs/en/english-vectors.html) | 17.22 | 5.77 | 19.99 |
| [dict2vec-300d.vec](https://github.com/tca19/dict2vec) | 12.71 | 4.04 | 13.04 |


| Word Embedding (l2) | EvalRank (MRR) | Hits1 | Hits3 |
| :--- | :---: | :---: | :---: |
| toy_emb.txt | xx | xx | xx |
| [glove.840B.300d.txt](https://nlp.stanford.edu/projects/glove/) | xx | xx | xx |
| [GoogleNews-vectors-negative300.txt](https://code.google.com/archive/p/word2vec/) | xx | xx | xx |
| [crawl-300d-2M.vec](https://fasttext.cc/docs/en/english-vectors.html) | xx | xx | xx |
| [dict2vec-300d.vec](https://github.com/tca19/dict2vec) | xx | xx | xx |


| Word Embedding (cos+post-processing) | EvalRank (MRR) | Hits1 | Hits3 |
| :--- | :---: | :---: | :---: |
| toy_emb.txt | xx | xx | xx |
| [glove.840B.300d.txt](https://nlp.stanford.edu/projects/glove/) | xx | xx | xx |
| [GoogleNews-vectors-negative300.txt](https://code.google.com/archive/p/word2vec/) | xx | xx | xx |
| [crawl-300d-2M.vec](https://fasttext.cc/docs/en/english-vectors.html) | xx | xx | xx |
| [dict2vec-300d.vec](https://github.com/tca19/dict2vec) | xx | xx | xx |


| Word Embedding (l2+post-processing) | EvalRank (MRR) | Hits1 | Hits3 |
| :--- | :---: | :---: | :---: |
| toy_emb.txt | xx | xx | xx |
| [glove.840B.300d.txt](https://nlp.stanford.edu/projects/glove/) | xx | xx | xx |
| [GoogleNews-vectors-negative300.txt](https://code.google.com/archive/p/word2vec/) | xx | xx | xx |
| [crawl-300d-2M.vec](https://fasttext.cc/docs/en/english-vectors.html) | xx | xx | xx |
| [dict2vec-300d.vec](https://github.com/tca19/dict2vec) | xx | xx | xx |


** In the previous version of our paper, Table 5 (a) used the wrong version without augmentation with wiki vocabulary. The result is shown as below: **

| Word Embedding (cos) w/o wiki vocab | EvalRank (MRR) | Hits1 | Hits3 |
| :--- | :---: | :---: | :---: |
| toy_emb.txt | xx | xx | xx |
| [glove.840B.300d.txt](https://nlp.stanford.edu/projects/glove/) | xx | xx | xx |
| [GoogleNews-vectors-negative300.txt](https://code.google.com/archive/p/word2vec/) | xx | xx | xx |
| [crawl-300d-2M.vec](https://fasttext.cc/docs/en/english-vectors.html) | xx | xx | xx |
| [dict2vec-300d.vec](https://github.com/tca19/dict2vec) | xx | xx | xx |